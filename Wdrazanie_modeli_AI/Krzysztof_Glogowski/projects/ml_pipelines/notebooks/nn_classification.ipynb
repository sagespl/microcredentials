{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b68f4a-ada0-404f-abb8-879d54d311e3",
   "metadata": {},
   "source": [
    "# Assumptions:\n",
    "\n",
    "Only documents up to 2 pages are considered, because:\n",
    "\n",
    "- most invoices are one-pagers,\n",
    "- pages 2, 3, 4 of multipages invoices are usually similar to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2d1cf-f77a-4744-ad40-6437dccccb68",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8b598-77b5-4a7c-8ab6-92be7cff71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ALBUMENTATIONS_DISABLE_UPDATE_CHECK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beedce5-345e-416e-a754-3ca3a89d429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Callable\n",
    "\n",
    "import albumentations as A\n",
    "import ast\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from enum import Enum\n",
    "from fsspec import AbstractFileSystem\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    "    MulticlassF1Score,\n",
    ")\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf31ba-705f-4f72-8951-b54447a8c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dbc1cf-98b1-4164-8e15-e839a926fefc",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628aebf-1307-4d67-90b0-110652ec9f0d",
   "metadata": {},
   "source": [
    "## Clear cuda cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f255e-aa5a-4607-b3b3-12e4ba03a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc2cc4-d26b-43a7-bc67-602a24bab441",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2755e72-d38c-4584-a8fd-25e6ebf4b2d7",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53ee3a-1dd2-45e6-abc0-09663f097833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filesystem(\n",
    "    path: str, fs_args: dict | None = None, credentials: dict | None = None\n",
    ") -> AbstractFileSystem:\n",
    "    _fs_args = deepcopy(fs_args) or {}\n",
    "    _credentials = deepcopy(credentials) or {}\n",
    "\n",
    "    protocol = \"file\"  # constant for local development\n",
    "    if protocol == \"file\":\n",
    "        _fs_args.setdefault(\"auto_mkdir\", True)\n",
    "\n",
    "    return fsspec.filesystem(protocol, **{**_credentials, **_fs_args})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda75df-d12c-4470-8dac-901b5cb7d0f4",
   "metadata": {},
   "source": [
    "## Definition of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9f981-f262-4dd7-bd8d-dbd76ab61503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSequencesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath: str,\n",
    "        paths_column: str = \"paths\",\n",
    "        label_column: str = \"label\",\n",
    "        fs_args: dict | None = None,\n",
    "        credentials: dict | None = None,\n",
    "        transform: Callable[[Image.Image], torch.Tensor] | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_file_path = filepath\n",
    "        self.paths_column = paths_column\n",
    "        self.label_column = label_column\n",
    "        self.data: pd.DataFrame = self._load()\n",
    "        self.transform_fn = transform\n",
    "        self._fs: AbstractFileSystem = get_filesystem(self.data_file_path, fs_args, credentials)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return 0 if self.data is None else len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[list[torch.Tensor], int]:\n",
    "        img_paths = self.data.loc[idx, self.paths_column]\n",
    "        label = self.data.loc[idx, self.label_column]\n",
    "        item = []\n",
    "\n",
    "        for img_path in img_paths:\n",
    "            with self._fs.open(img_path, \"rb\") as f:\n",
    "                img = Image.open(f)\n",
    "                item.append(self._transform(img))\n",
    "\n",
    "        return item, label\n",
    "\n",
    "    # helpers\n",
    "    def _load(self) -> pd.DataFrame:\n",
    "        data = pd.read_csv(self.data_file_path)\n",
    "        data[self.paths_column] = data[self.paths_column].apply(ast.literal_eval)\n",
    "        return data\n",
    "\n",
    "    def _transform(self, img: Image.Image) -> torch.Tensor:\n",
    "        if self.transform_fn is not None:\n",
    "            return self.transform_fn(img)\n",
    "        return A.ToTensorV2()(image=np.array(img))[\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45618672-6f7c-4e43-972f-7286a3f77e74",
   "metadata": {},
   "source": [
    "## Collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aea541-b7a1-438a-a13e-1731dd0e0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(\n",
    "    batch: list[tuple[list[torch.Tensor], int]],\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    items_lengths = np.array([len(items_pair[0]) for items_pair in batch])\n",
    "    sorted_indices = np.argsort(-items_lengths)  # sort in descending order\n",
    "\n",
    "    items_batch = []\n",
    "    items_label_batch = []\n",
    "\n",
    "    for idx in sorted_indices:\n",
    "        items, label = batch[idx]\n",
    "        items_batch += [*items]\n",
    "        items_label_batch.append(label)\n",
    "\n",
    "    return (\n",
    "        torch.stack(items_batch),\n",
    "        torch.LongTensor(items_lengths[sorted_indices]),\n",
    "        torch.LongTensor(items_label_batch),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9c9e3-37bb-4e2f-b66e-b73bf0c2b35d",
   "metadata": {},
   "source": [
    "# Image transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82762f9-d46e-4677-8b7d-8dd6a014b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img: Image.Image) -> torch.Tensor:\n",
    "    _transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=224, width=224),\n",
    "            A.GaussianBlur(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.ToGray(p=0.5),\n",
    "            A.Normalize(\n",
    "                mean=(0.5, 0.5, 0.5),\n",
    "                std=(0.5, 0.5, 0.5),\n",
    "            ),\n",
    "            A.ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return _transform(image=np.array(img))[\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ade2d4-d902-4105-b5f3-f26f3bacbc4c",
   "metadata": {},
   "source": [
    "# Test pretrained timm model (encoder backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c7133-bfb0-46c8-800c-ac5ce63d5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"timm/vit_base_patch16_clip_224.openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268c21b-65a0-40c4-bc26-e714531c48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model(model_name, pretrained=True, num_classes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8cd92-9adb-42a3-aaa5-8e8a95d06caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backbone??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c22ee3-4a2c-47ca-a9fb-e2172b0c438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../data/05_model_input/notebooks/train/invoice_0_type_2/invoice_0_type_2_0.jpg\")\n",
    "inputs = transform(image)\n",
    "\n",
    "# Get the model's output\n",
    "with torch.no_grad():\n",
    "    outputs = backbone(inputs.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f68bb0-8c41-4159-b9fc-57c3cd9462b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda5f93-267c-4cc8-9f22-c4b7174b237a",
   "metadata": {},
   "source": [
    "# Image Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10471dcc-236f-48de-9bc9-a64e9fc44379",
   "metadata": {},
   "source": [
    "## Recursive Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721cd03-bdfa-435a-a6b7-6f998754fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveImageEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        hidden_size: int = 512,\n",
    "        rnn_layers_num: int = 1,\n",
    "        dropout: float = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.embedding_dim = backbone.num_features\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=rnn_layers_num,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor):\n",
    "        positional_embeddings = self._get_positional_embeddings(max(lengths), self.embedding_dim)\n",
    "        output = self.backbone(x)\n",
    "        sequences = torch.split(output, lengths.tolist(), dim=0)\n",
    "        padded_sequences = pad_sequence(list(sequences), batch_first=True)\n",
    "        padded_sequences += positional_embeddings\n",
    "        packed_seqs = pack_padded_sequence(\n",
    "            padded_sequences, lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "        _, last_hidden_state = self.rnn(packed_seqs)\n",
    "\n",
    "        return last_hidden_state.squeeze(dim=0)\n",
    "\n",
    "    def _get_positional_embeddings(self, seq_len, embedding_dim):\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embedding_dim, 2) * (-torch.log(torch.tensor(10000.0)) / embedding_dim)\n",
    "        )\n",
    "        pe = torch.zeros(seq_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe541804-e5cc-4a58-805f-c62568e3c970",
   "metadata": {},
   "source": [
    "## Avg Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ab4c3-989c-4891-a1b3-d46af6e1458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgImageEncoder(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        output = self.backbone(x)\n",
    "        sequences = torch.split(output, lengths.tolist(), dim=0)\n",
    "        batch_output = torch.stack([torch.mean(sec, dim=0) for sec in sequences])\n",
    "\n",
    "        return batch_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753bf5c0-ef34-409a-a62d-efaa0ff3ba45",
   "metadata": {},
   "source": [
    "# Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c00920-3f21-40ac-8d4f-ab032f33960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_shape, output_size, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Sequential(\n",
    "            *self.__build_hidden_layers(input_size, hidden_shape, activation)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_shape[-1], output_size)\n",
    "\n",
    "    def __build_hidden_layers(self, input_size, hidden_shape, activation):\n",
    "        hidden_layers = [nn.Linear(input_size, hidden_shape[0]), activation]\n",
    "        for i in range(0, len(hidden_shape) - 1):\n",
    "            hidden_layers.extend([nn.Linear(hidden_shape[i], hidden_shape[i + 1]), activation])\n",
    "        return hidden_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534dd24-d91a-470d-9f97-4d9793217113",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38c89d-f9a8-45c6-84cb-6cdcb3abd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(self, encoder: AvgImageEncoder | RecursiveImageEncoder, classifier: MLP):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.encoder(*x)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acd3a1-af6c-4a99-a99b-7c6273c95b7d",
   "metadata": {},
   "source": [
    "# ClassificationReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc88b4-9c78-4039-b0cc-0e75712ce84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationReport:\n",
    "    def __init__(self, num_classes: int = 4, average: str = \"macro\"):\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "        self.precision = MulticlassPrecision(num_classes=num_classes, average=average).to(device)\n",
    "        self.recall = MulticlassRecall(num_classes=num_classes, average=average).to(device)\n",
    "        self.f1 = MulticlassF1Score(num_classes=num_classes, average=average).to(device)\n",
    "\n",
    "    def generate(self, y_pred, y_true):\n",
    "        accuracy_score = self.accuracy(y_pred, y_true)\n",
    "        precision_score = self.precision(y_pred, y_true)\n",
    "        recall_score = self.recall(y_pred, y_true)\n",
    "        f1_score = self.f1(y_pred, y_true)\n",
    "\n",
    "        return accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2af50-fe82-4041-a6a0-1b4fd848856d",
   "metadata": {},
   "source": [
    "# Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52ddcb-41fe-4ee6-bc7f-fddff5032413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    classifiaction_report = ClassificationReport()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        epoch_loss = 0\n",
    "        y_true = torch.empty(0).to(device)\n",
    "        y_pred = torch.empty(0).to(device)\n",
    "\n",
    "        for items, lengths, labels in tqdm(dataloader):\n",
    "            items, lengths, labels = (\n",
    "                items.to(device),\n",
    "                lengths.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            output = model((items, lengths))\n",
    "            _, lables_pred = output.max(dim=1)\n",
    "\n",
    "            y_true = torch.cat((y_true, labels), dim=0)\n",
    "            y_pred = torch.cat((y_pred, lables_pred), dim=0)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            epoch_loss += loss.item() * items.size(0)\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "    accuracy, precision, recall, f1 = classifiaction_report.generate(y_pred, y_true)\n",
    "    return avg_epoch_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a509fb5-fe17-4d89-aa02-6735f1edfce4",
   "metadata": {},
   "source": [
    "# Zero-shot tests (to check whether encoder and classifier work together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b9ea1-a637-48cc-b76e-2fc4f4e17021",
   "metadata": {},
   "source": [
    "## Encoder backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850efe81-6034-452b-9d75-00da63e96966",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model(\n",
    "    \"timm/vit_base_patch16_clip_224.openai\", pretrained=True, num_classes=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058852c-d82b-4190-9c4c-7ff3d1f9b482",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d7ae1-da72-4bd8-82dd-4df933aab657",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageSequencesDataset(\n",
    "    filepath=\"../data/05_model_input/notebooks/train/metadata.csv\", transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9124c8-b1f0-4ad4-aaca-e6ce7d54c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51389585-ce31-47af-af26-0e46448838b6",
   "metadata": {},
   "source": [
    "## Tests with Recursive Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119882cf-f178-490b-abb5-52132f58a288",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966464c5-6195-4544-8831-23fb7eb763aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_rnn = RecursiveImageEncoder(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de15c665-f93d-43cf-a80d-5c9eea4a1dd6",
   "metadata": {},
   "source": [
    "### Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d947099-686b-44db-b91a-a85c1a837555",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_rnn = MLP(512, (512, 256, 64, 16), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88814f-61fb-4681-a216-a43f651d3a66",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb46bd-1f61-4dd9-811e-495d96c6ae80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_rnn = DocumentClassifier(encoder_rnn, classification_head_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28ee57-5bc1-4b05-8e92-e07c23a56481",
   "metadata": {},
   "source": [
    "### Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ababe2-d24c-477b-8144-4ad59642baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.eval()\n",
    "with torch.no_grad():\n",
    "    model_rnn.to(device)\n",
    "    for batch in tqdm(dataloader):\n",
    "        items, lengths, labels = batch\n",
    "        items = items.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        model_rnn((items, lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c17b3-3f55-4ed4-b93d-d405874a0275",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ceb38c-6b18-4616-b284-ad1c10869872",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, precision, recall, f1 = evaluate(model_rnn, dataloader, CrossEntropyLoss())\n",
    "loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac613510-7154-4088-9543-596775ebeb20",
   "metadata": {},
   "source": [
    "## Test with Avg Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285132f-97f0-43c4-9faa-c038b0b5e01e",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b6d4e-a101-4484-b43e-95d13ac8a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_avg = AvgImageEncoder(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d362347-0217-49d4-b63c-489b77422b79",
   "metadata": {},
   "source": [
    "### Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a6c6b-3a1a-4f52-9f04-59c8491b3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_avg = MLP(768, (265, 16, 8), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3be7a-bbc2-4e85-95ec-357f9615cf47",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90705b5-2881-4eea-96d6-d829debf87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_avg = DocumentClassifier(encoder_avg, classification_head_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be060e0a-a752-4d46-8d55-e8ef75833fed",
   "metadata": {},
   "source": [
    "### Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00911b8-9076-4bfb-a04d-83231a0e1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_avg.eval()\n",
    "with torch.no_grad():\n",
    "    model_avg.to(device)\n",
    "    for batch in tqdm(dataloader):\n",
    "        items, lengths, labels = batch\n",
    "        items = items.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        model_avg((items, lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285af4b-f258-4e2a-954d-c493817c6b12",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eafd8e-e598-4e5d-bcca-08284b62249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, precision, recall, f1 = evaluate(model_avg, dataloader, CrossEntropyLoss())\n",
    "loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2ff29-5c6d-48fb-a195-0d1eda7e8676",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The zero-shot approach is not the correct approach, and models need to be trained, as clearly shown by all metrics. However, it has been proven that all components of the final model work together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812e642-5576-49a6-8fe8-708f7b754e53",
   "metadata": {},
   "source": [
    "# Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a6ea1-fe70-4381-9941-f121fbf799e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience: int = 1, min_delta: float = 0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_stop_metric_value = float(\"inf\")\n",
    "        self.best_model_state_dict: dict | None = None\n",
    "\n",
    "    def early_stop(self, metric_value: float, model: nn.Module) -> bool:\n",
    "        if metric_value < self.min_stop_metric_value:\n",
    "            self.min_stop_metric_value = metric_value\n",
    "            self.counter = 0\n",
    "            self.best_model_state_dict = deepcopy(model.state_dict())\n",
    "        elif metric_value > (self.min_stop_metric_value + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e015b-f048-4509-9646-21d4861c4bcc",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134b5b3-039c-4a92-8a1b-792e9d31a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopMetric(Enum):\n",
    "    LOSS = 0\n",
    "    ACCURACY = 1\n",
    "    PRECISION = 2\n",
    "    RECALL = 3\n",
    "    F1 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83967f8e-81ef-46f3-b522-dba49c8e8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freez_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874b931-27c2-4c85-8849-5ab501a615b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, optimizer, criterion):\n",
    "    classifiaction_report = ClassificationReport()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    y_true = torch.empty(0).to(device)\n",
    "    y_pred = torch.empty(0).to(device)\n",
    "\n",
    "    for items, lengths, labels in tqdm(train_dataloader):\n",
    "        items, lengths, labels = items.to(device), lengths.to(device), labels.to(device)\n",
    "        output = model((items, lengths))\n",
    "        _, lables_pred = output.max(dim=1)\n",
    "\n",
    "        y_true = torch.cat((y_true, labels), dim=0)\n",
    "        y_pred = torch.cat((y_pred, lables_pred), dim=0)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * items.size(0)\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "    accuracy, precision, recall, f1 = classifiaction_report.generate(y_pred, y_true)\n",
    "    return avg_epoch_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a75780-8f07-48ef-8ee5-4eb9d6156dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, optimizer, criterion, epochs, stop_metric):\n",
    "    early_stopper = EarlyStopper(20)\n",
    "    freez_model(model.encoder.backbone)\n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        metrics_train = train_one_epoch(model, train_dataloader, optimizer, criterion)\n",
    "        metrics_valid = evaluate(model, valid_dataloader, criterion)\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(\n",
    "            f\"loss_train={metrics_train[0]}, accuracy_train={metrics_train[1]}, precision_train={metrics_train[2]}, recall_train={metrics_train[3]}, f1_train={metrics_train[4]}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"loss_valid={metrics_valid[0]}, accuracy_valid={metrics_valid[1]}, precision_valid={metrics_valid[2]}, recall_valid={metrics_valid[3]}, f1_valid={metrics_valid[4]}\"\n",
    "        )\n",
    "        if early_stopper.early_stop(metrics_valid[stop_metric.value], model):\n",
    "            print(\n",
    "                f\"Early stopping on epoch {epoch}, {stop_metric.name}: {metrics_valid[stop_metric.value]}\"\n",
    "            )\n",
    "            break\n",
    "    if early_stopper.best_model_state_dict is not None:\n",
    "        model.load_state_dict(early_stopper.best_model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70175e79-9baf-420a-9912-46ef70bb7b0d",
   "metadata": {},
   "source": [
    "# Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452ef53-2612-4fb0-a96e-e563bdfaa20e",
   "metadata": {},
   "source": [
    "## Encoder backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df6c35-dcc2-4945-8401-009c0d5f63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model(\n",
    "    \"timm/vit_base_patch16_clip_224.openai\", pretrained=True, num_classes=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728f39f-d7a4-4490-afc5-1ad8b6a9eeb7",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51482312-c44e-4515-8c8a-84deebdf4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ImageSequencesDataset(\n",
    "    filepath=\"../data/05_model_input/notebooks/train/metadata.csv\", transform=transform\n",
    ")\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=20, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493993f3-e4f4-4168-bb06-4c0deb69bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = ImageSequencesDataset(\n",
    "    filepath=\"../data/05_model_input/notebooks/valid/metadata.csv\", transform=transform\n",
    ")\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=10, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc24092-9e27-4c99-8aa3-bbb78324eebc",
   "metadata": {},
   "source": [
    "## Train Classifer with Recursive Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b453a5-2d2a-435d-bffd-2909c9337471",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a799ab-4aa2-4115-996b-811e3df040e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_rnn = RecursiveImageEncoder(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e50ad40-aa62-4f81-bf9a-50d1ee2af1ca",
   "metadata": {},
   "source": [
    "### Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0221a79-0b81-4c84-9dbd-c5bbda7f7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_rnn = MLP(512, (512, 256, 64, 16), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed396b05-53c3-4d58-bd87-e52c4b6c014f",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51182c36-f6a5-4a09-a4a5-754795385cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = DocumentClassifier(encoder_rnn, classification_head_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c82b35-7cbd-490f-9eb0-c794f30ac376",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c864c14-58d8-4c75-8697-4f1e96db2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model_rnn,\n",
    "    dataloader_train,\n",
    "    dataloader_valid,\n",
    "    optim.Adam(model_rnn.parameters(), lr=0.0000001),\n",
    "    CrossEntropyLoss(),\n",
    "    10,\n",
    "    StopMetric.LOSS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7cb7c-e42c-4222-bcb1-3222477371c5",
   "metadata": {},
   "source": [
    "## Train Classifier with Avg Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ca04b-9b81-4690-82d8-37e61f215f45",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fcf53f-505a-4cf2-9ce7-1fb8c77dfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_avg = AvgImageEncoder(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca138a5f-075e-41d1-8f59-74f93b37f402",
   "metadata": {},
   "source": [
    "### Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214eb384-6b93-40e7-bbaa-ac528e1a224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_avg = MLP(768, (265, 16, 8), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097c430-6a7b-4b85-be17-993747a315c4",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75d99a-f35e-4559-b5f1-b16dd00a0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_avg = DocumentClassifier(encoder_avg, classification_head_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb94d5d-65b3-423f-8f34-e758f519d0fb",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d31be-d14e-45f5-9e9e-824699079396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    model_avg,\n",
    "    dataloader_train,\n",
    "    dataloader_valid,\n",
    "    optim.Adam(model_avg.parameters(), lr=0.00001),\n",
    "    CrossEntropyLoss(),\n",
    "    10,\n",
    "    StopMetric.LOSS,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
